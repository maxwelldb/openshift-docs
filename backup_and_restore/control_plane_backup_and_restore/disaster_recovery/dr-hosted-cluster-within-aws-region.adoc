:_content-type: ASSEMBLY
[id="dr-hosted-cluster-within-aws-region"]
= Disaster recovery for a hosted cluster within an AWS region
include::_attributes/common-attributes.adoc[]
:context: dr-hosted-cluster-within-aws-region

toc::[]

In a situation where you need disaster recovery (DR) for a hosted cluster, you can recover a hosted cluster to the same region within AWS. For example, you need DR when the upgrade of a management cluster fails and the hosted cluster is in a read-only state.

:FeatureName: Hosted control planes
include::snippets/technology-preview.adoc[]

The DR process involves three main steps:

. Backing up the hosted cluster on the source management cluster
. Restoring the hosted cluster on a destination management cluster
. Deleting the hosted cluster from the source management cluster

Your workloads remain running during the process. The Cluster API might be unavailable for a period, but that will not affect the services that are running on the worker nodes.

[IMPORTANT]
====
Both the source management cluster and the destination management cluster must have the `--external-dns` flags to maintain the API server URL, as shown in this example:

.Example: External DNS flags
[source,terminal]
----
--external-dns-provider=aws \
--external-dns-credentials=<AWS Credentials location> \
--external-dns-domain-filter=<DNS Base Domain>
----

That way, the server URL ends with `https://api-sample-hosted.sample-hosted.aws.openshift.com`.

If you do not include the `--external-dns` flags to maintain the API server URL, the hosted cluster cannot be migrated.
====

[id="dr-hosted-cluster-env-context"]
== Example environment and context

Consider an scenario where you have three clusters to restore. Two are management clusters, and one is a hosted cluster. You can restore either the control plane only or the control plane and the nodes. Before you begin, you need the following information:

* Source MGMT Namespace: The source management namespace
* Source MGMT ClusterName: The source management cluster name
* Source MGMT Kubeconfig: The source management `kubeconfig` file
* Destination MGMT Kubeconfig: The destination management `kubeconfig` file
* HC Kubeconfig: The hosted cluster `kubeconfig` file
* SSH key file: The SSH public key
* Pull secret: The pull secret file to access the release images
* AWS credentials
* AWS region
* Base domain: The DNS base domain to use as an external DNS
* S3 bucket name: The bucket in the AWS region where you plan to upload the etcd backup

This information is shown in the following example environment variables.

.Example environment variables
[source,terminal]
----
SSH_KEY_FILE=${HOME}/.ssh/id_rsa.pub
BASE_PATH=${HOME}/hypershift
BASE_DOMAIN="aws.sample.com"
PULL_SECRET_FILE="${HOME}/pull_secret.json"
AWS_CREDS="${HOME}/.aws/credentials"
AWS_ZONE_ID="Z02718293M33QHDEQBROL"

CONTROL_PLANE_AVAILABILITY_POLICY=SingleReplica
HYPERSHIFT_PATH=${BASE_PATH}/src/hypershift
HYPERSHIFT_CLI=${HYPERSHIFT_PATH}/bin/hypershift
HYPERSHIFT_IMAGE=${HYPERSHIFT_IMAGE:-"quay.io/${USER}/hypershift:latest"}
NODE_POOL_REPLICAS=${NODE_POOL_REPLICAS:-2}

# MGMT Context
MGMT_REGION=us-west-1
MGMT_CLUSTER_NAME="${USER}-dev"
MGMT_CLUSTER_NS=${USER}
MGMT_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT_CLUSTER_NS}-${MGMT_CLUSTER_NAME}"
MGMT_KUBECONFIG="${MGMT_CLUSTER_DIR}/kubeconfig"

# MGMT2 Context
MGMT2_CLUSTER_NAME="${USER}-dest"
MGMT2_CLUSTER_NS=${USER}
MGMT2_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${MGMT2_CLUSTER_NS}-${MGMT2_CLUSTER_NAME}"
MGMT2_KUBECONFIG="${MGMT2_CLUSTER_DIR}/kubeconfig"

# Hosted Cluster Context
HC_CLUSTER_NS=clusters
HC_REGION=us-west-1
HC_CLUSTER_NAME="${USER}-hosted"
HC_CLUSTER_DIR="${BASE_PATH}/hosted_clusters/${HC_CLUSTER_NS}-${HC_CLUSTER_NAME}"
HC_KUBECONFIG="${HC_CLUSTER_DIR}/kubeconfig"
BACKUP_DIR=${HC_CLUSTER_DIR}/backup

BUCKET_NAME="${USER}-hosted-${MGMT_REGION}"

# DNS
AWS_ZONE_ID="Z07342811SH9AA102K1AC"
EXTERNAL_DNS_DOMAIN="hc.jpdv.aws.kerbeross.com"
----

[id="dr-hosted-cluster-process"]
== Overview of the backup and restore process

The backup and restore process works as follows:

. On management cluster 1, which you can think of as the source management cluster, the control plane and workers interact by using the ExternalDNS API.

. You take a snapshot of the hosted cluster, which includes etcd, the control plane, and the worker nodes. The worker nodes are moved to the external DNS, the control plane is saved in a local manifest file, and etcd is backed up to an S3 bucket.

. On management cluster 2, which you can think of as the destination management cluster, you restore etcd from the S3 bucket and restore the control plane from the local manifest file.

. By using the External DNS API, the worker nodes are restored to management cluster 2.

. On management cluster 2, the control plane and worker nodes interact by using the ExternalDNS API.

// When the updated diagram is available, I will add it here and update the first sentence in this section to read, "As shown in the following diagram, the backup and restore process works as follows:"

You can manually back up and restore your hosted cluster, or you can run a script to complete the process. For more information about the script, see "Running a script to back up and restore a hosted cluster".

// Backing up the hosted cluster
include::modules/dr-hosted-cluster-within-aws-region-backup.adoc[leveloffset=+1]

// Restoring the hosted cluster
include::modules/dr-hosted-cluster-within-aws-region-restore.adoc[leveloffset=+1]

// Deleting the hosted cluster
include::modules/dr-hosted-cluster-within-aws-region-delete.adoc[leveloffset=+1]

//Helper script
include::modules/dr-hosted-cluster-within-aws-region-script.adoc[leveloffset=+1]